{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6gHiH-I7uFa"
   },
   "source": [
    "# Improving Computer Vision Accuracy using Convolutions\n",
    "\n",
    "In the videos you looked at how you would improve Fashion MNIST using Convolutions. For your exercise see if you can improve MNIST to 99.8% accuracy or more using only a single convolutional layer and a single MaxPooling 2D. You should stop training once the accuracy goes above this amount. It should happen in less than 20 epochs, so it's ok to hard code the number of epochs for training, but your training must end once it hits the above metric. If it doesn't, then you'll need to redesign your layers.\n",
    "\n",
    "I've started the code for you -- you need to finish it!\n",
    "\n",
    "When 99.8% accuracy has been hit, you should print out the string \"Reached 99.8% accuracy so cancelling training!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaLX5cgI_JDb"
   },
   "source": [
    "# Step 1: Load the dataset\n",
    "\n",
    "The dataset is already available in tensorflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Splitting the data into training and testing\n",
    "\n",
    "You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SS_W_INc_kJQ"
   },
   "source": [
    "# Step 3: Next is to define your model. \n",
    "\n",
    "Now instead of the input layer at the top, you're going to add a Convolution. The parameters are:\n",
    "\n",
    "1. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32\n",
    "2. The size of the Convolution, in this case a 3x3 grid\n",
    "3. The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0\n",
    "4. In the first layer, the shape of the input data.\n",
    "\n",
    "You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image by 25%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch, logs={}):\n",
    "        if(logs.get('acc')>=0.998):\n",
    "            print(\"\\n Reached 99.8% accuracy, so cancelling training\")\n",
    "            self.model.stop_training = True\n",
    "    \n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 33s 553us/sample - loss: 0.1205 - acc: 0.9637\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 33s 556us/sample - loss: 0.0411 - acc: 0.9873\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 34s 561us/sample - loss: 0.0286 - acc: 0.9909\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 34s 567us/sample - loss: 0.0209 - acc: 0.9932\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 35s 580us/sample - loss: 0.0155 - acc: 0.9951\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 35s 591us/sample - loss: 0.0125 - acc: 0.9957\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 36s 601us/sample - loss: 0.0089 - acc: 0.9972\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 42s 699us/sample - loss: 0.0082 - acc: 0.9973\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 39s 644us/sample - loss: 0.0070 - acc: 0.9978\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 42s 692us/sample - loss: 0.0073 - acc: 0.9976\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 41s 685us/sample - loss: 0.0059 - acc: 0.9979 - loss: 0.0059 - acc - ETA: 1s - l\n",
      "Epoch 12/20\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9985\n",
      " Reached 99.8% accuracy, so cancelling training\n",
      "60000/60000 [==============================] - 38s 638us/sample - loss: 0.0043 - acc: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14c8c2324c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_images, training_labels, epochs=20, callbacks = [callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 145us/sample - loss: 0.0388 - acc: 0.9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.038763871919828566, 0.9916]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**\n",
    "\n",
    "It returned a accuracy of about .9916, which means it was about 99% accurate. As expected it probably would not do as well with unseen data as it did with data it was trained on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXx_LX3SAlFs"
   },
   "source": [
    "# Visualizing the Convolutions and Pooling\n",
    "\n",
    "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "f-6nX4QsOku6",
    "outputId": "6b85ed93-6868-4c2c-b066-0808d6536878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7\n",
      " 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9\n",
      " 1 7 3 2 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "9FGsHhv6JvDx",
    "outputId": "e144d639-cebc-4d0a-9c7a-8571f70d6159"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, axarr = plt.subplots(3,4)\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=7\n",
    "THIRD_IMAGE=26\n",
    "CONVOLUTION_NUMBER = 1\n",
    "from tensorflow.keras import models\n",
    "layer_outputs = [layer.output for layer in model.layers]   #each layer output\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "for x in range(0,4):\n",
    "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[0,x].grid(False)\n",
    "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[1,x].grid(False)\n",
    "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Course 1 - Part 6 - Lesson 2 - Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
